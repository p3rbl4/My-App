**RAID** (Redundant Array of Independent Disks_ — избыточный массив _независимых (самостоятельных) дисков).
Он был придуман для объединения нескольких физических дисковых устройств в логический модуль для повышения отказоустойчивости и производительности.

**RAID**-массивы бывают базовые, комбинированные и нестандартные.

Базовые уровни **RAID**

1. **RAID 0** или **stripe**. Дисковый массив повышенной производительности, но без отказоустойчивости.
Запись и чтение данных чередуется между дисками для обеспечения максимальной скорости.
Емкость массива равняется емкости входящих в него жестких дисков.
Если хоть один из дисков выходит из строя то восстановить данные скорее всего невозможно.
Поэтому применяется такой массив там, где надо очень быстро выдавать статическую информацию, которую всегда можно на свежий массив развернуть из бекапа.

2. **RAID 1** или зеркало.
Дисковый массив повышенной отказоустойчивости. Фактически все изменения пишутся на все диски массива одновременно. Т.е. если один из дисков выходит из строя, то на работе сервера это не скажется и будет время на замену.
Диски в массиве должны быть одинакового размера. Размер топа равен размеру одного диска.
В целом если диски не из одной партии, то некоторая защита есть. Пока есть хоть один живой жесткий диск - данные вытащить не составит труда. **Но надо помнить, что это не бэкап.** Если серверную затопит, то помрут все диски. Если сын главного бухгалтера забежит в серверную с магнитом, то он может испортить информацию на всех жестких дисках. Если будет пожар, то информацию тоже не спасти. Так что резервные копии делать все равно надо.
**Минусами** этого типа массива являются емкость и скорость. Поэтому были разработаны следующие виды массивов.

3. **RAID 3** и **4**(почти нигде не используется) - дисковые массивы с чередованием и выделенным диском четности.
4. **RAID 5** дисковый массив с чередованием и отсутствием выделенного диска чётности.
Этот массив учитывает недостатки предыдущих. Коды коррекции тут тоже используются, но раскиданы по всем дискам.
Т.е. нет единственного диска, чей отказ поставит под удар весь массив. При выходе из строя любого из дисков массив продолжит работать и данные не потеряются.
Минимальное количество дисков для штатного состояния - 4. Минимальное количество дисков для сохранения работоспособности 3.
Суммарный объем массива, если у нас **N**-дисков - (**N - 1)***размер_диска.
Благодаря параллельной работе со всеми дисками достигается хорошая скорость дисковых операций. Довольно долго этот тип массива был самым популярным за его скорость и экономичность по части дисков.

5. **RAID 6** - дисковый массив с чередованием, использующий две контрольные суммы, вычисляемые двумя независимыми способами.
Этот тип массива стал логичным развитием предыдущего. Если контрольные суммы в **RAID 5** вычисляются только с помощью **XOR**, то в **RAID 6** вычисляются две контрольные суммы (алгоритм основан на кодах Рида - Соломона). Это дает еще большую надежность. При этом скорость работы массива будет чуть меньше.
Минимальное количество дисков 5. Сохраняет работоспособность при выходе из строя любых двух дисков.

Есть также комбинированные RAID, к примеру сейчас один из самых используемых это **RAID 10 (1+0)** страйп из двух зеркал (очень популярен, т.к. и массив быстрый и восстановление данных по сравнению с RAID 5,6 тоже максимально быстрое

# Для управления **RAID**-массивами в Linux используется утилита **mdadm**

Синтаксис утилиты mdadm выглядит следующим образом 
`mdadm [режим] [массив] [доп. параметры]

Режимы бывают следующие:

-A, --assemble - режим сборки массива (уже когда-то созданного)

-B, --build - режим построения

-C, --create - режим создания

-F, --follow, --monitor - режим наблюдения

-G, --grow - режим расширения

-I, --incremental - режим инкрементальной сборки

**Массив**, в данном случае, это перечисление дисков, которые будут включены в массив

**Доп. параметры** в схеме указаны последними, но их можно размещать в любом месте команды.

Перед созданием массива, нужно убедиться все ли диски имею одинаковые разделы, если нет, то лучше создать одинаковые разделы на всех дисках которые буду включены в массив. 

Создадим сейчас **RAID 5** на примере 3 заранее размеченных флешек

Команда на сборку массива выглядит следующим образом 
```bash
mdadm --create --verbose /dev/md0 --level=5 --raid-devices=3 /dev/sdb1 /dev/sdc1 /dev/sdd1
```
1.  --create: это опция, которая сообщает утилите mdadm, что мы хотим создать новый RAID-массив.
2.  --verbose: это опция, которая сообщает утилите mdadm, что нужно выводить подробную информацию о создании массива в терминал.
3.  /dev/md0: это путь к устройству, на которое будет смонтирован RAID-массив. (то есть это массив, который будет создан)
4.  --level=5: это опция, которая указывает на уровень RAID-массива. Уровень 5 означает, что данные будут распределены между тремя дисками (raid-devices) с использованием паритетной информации. Это обеспечивает устойчивость к отказам одного из дисков.
5.  --raid-devices=3: это опция, которая указывает количество дисков, которые будут использоваться для создания RAID-массива. В данном случае, мы используем три диска (/dev/sdb1, /dev/sdc1, /dev/sdd1).
6.  /dev/sdb1, /dev/sdc1, /dev/sdd1: это пути к устройствам, которые будут использоваться для создания RAID-массива. Каждый из них должен быть указан в отдельности.

**Первое** что всегда смотрят при диагностике **raid**-массивов - файл **/proc/mdstat**. В нем отображаетя состояние всех собранных массивов.

Мы видим наш массив **md0**. Перечислены его параметры и ниже строка похожая на прогрессбар. По сути это он и есть. Все дисковые разделы хранят разную информацию изначально. Тут показан процесс синхронизации. В это время дисковый массив (да и вся система ввода вывода сервера) может сильно тормозить.
`[3/2] [UU_]` это значит что два диска нормальные, а один проходит с ними синхронизацию. Такая картина наблюдается, когда сервер резко выключился по питанию, либо когда вставляешь новый диск в массив, взамен вышедшего из строя.

После некоторого времени можно проверить созданный массив в выводе команды fdisk -l 
![[Pasted image 20230311212431.png]]

По данным всё верно. В массив добавлено 3 диска по 6Гб. Емкость массива это n-1 * 6Гб, т.е. 12Гб.

**Далее**

Массив готов и представлен как жёсткий диск, нужно разметить на нём файловую систему и записать пару файлов. 

Создаём папку которая будет выделена для массива, размечаем и примаунчиваем её.
```bash
sudo mkdir /mnt/tmp

sudo mkfs.ext4 /dev/md0

sudo mount /dev/md0 /mnt/tmp
```
Можно перед монтированием добавить сразу диск и папку в файл /etc/fstab, но c массивами это работает чуть иначе. **Читай ниже** 
/dev/md0 /mnt/tmp ext4 defaults 0 0


# Создаём конфигурационный файл для mdadm

Это важный момент, т.к. автоматически информация о созданных массивах нигде не сохраняется. И чтобы ОС при загрузке знала какие массивы собрать и из каких дисков/разделов, мы и создаем этот конфиг.

Конфиг расположен вот по этому адресу: **/etc/mdadm/mdadm.conf**
Если его нет, то инициализируем следующим образом 
```bash
echo "DEVICE partitions" > /etc/mdadm/mdadm.conf
```

Теперь сбрасываем информацию о всех массивах в тот же конфиг, лучше делать это из под root, так как перенаправление потока с использованием sudo работает не совсем корректно
```bash
mdadm --detail --scan --verbose | awk '/ARRAY/ {print}' >> /etc/mdadm/mdadm.conf
```

*После этого можно массив разбирать и собирать заново по желанию.*

***Чтобы остановить работу массива нужно выполнить***
```bash
mdadm -S /dev/md0
```

Проверяем в /proc/mdstat и там ничего быть не должно

***Как вернуть массив назад?***

Для этого есть режим **--assemble**. 
1 вариант - В автоматическом режиме 
```bash
sudo mdadm --assemble --scan
```

Этой командой соберутся все массивы из конфига и дополнительно программа просмотрит все диски и разделы на компьютере на предмет метаданных массивов. И если найдет что-то помимо конфига, то соберет и их.

2 вариант - В ручном режиме 
```bash
sudo mdadm --assemble /dev/md0 /dev/sdb1 /dev/sdc1 /dev/sdd1
```
Так мы собираем конкретный массив. Похоже немного на команду создания, но мы указываем только имена массива и дисков. Остальную информацию **mdadm** берет не из конфига. Он берет ее из метаданных, которые находятся в начале каждого диска, вхощядего в массив.

# Расширение массива

Допустим у нас на руках новый хард и мы вставили его в сервер (ПК) 

После того как разметили диск и он у нас отображается как обычный делаем следующее.

Добавляем новый диск в массив командой 
```bash
sudo mdadm /dev/md0 -a /dev/sdh1
```

Теперь расширяем массив на этот диск 
```bash
sudo mdadm -G /dev/md0 --raid-devices=4
```

После этого надо обновить конфиг **mdadm.conf**. Старую запись массива удалить, а новую добавить как было описано выше.

Если есть желание проверить целостность массива, то делается это так. 
```bash
echo 'check' > /sys/block/md0/md/sync_action

cat /proc/mdstat
```

В конце проверки результат можно будет увидеть вот такой командой
```bash
sudo cat /sys/block/md0/md/mismatch_cnt
```
Если там **0** , значит массив в порядке.

Если у тебя весь сервер тормозит из-за синхронизации или проверки массива с не очень критичными данными, то можно остановить этот процесс командой.
```bash
echo 'idle' > /sys/block/md0/md/sync_action
```
А в ночь, когда с сервером никто не работает, запустить заново той же командой, которая была в предыдущем примере.

# Выход из строя одного из дисков в массиве

В этом случае нужно из массива удалить этот диск и добавить туда же новый. Расширение в этом случае не нужно, т.к. количество дисков останется тем же.

Как увидеть какой из дисков сдох?

Есть команда для подробного вывода информации о массиве ( **-D - debug**)

```bash
sudo mdadm -D /dev/md0
```

Если плохо то в конце вывода будет ***failed*** в столбце State, если всё ок то ***Active***

*С другой стороны все диски могут быть в порядке, но, к примеру, один из них работает уже 10 лет и его надо заменить по регламенту.*

Ситуация сильно не меняется. Нужно удалить диск из массива.

Делается это так 
```bash
sudo mdadm /dev/md0 --remove /dev/sdc1
```

После замены диска выполняем.
```bash
sudo mdadm -a /dev/md0 /dev/sdc1
```
Диск добавится и автоматически начнется синхронизация.


Если скорость синхронизации кажется медленной и ты думаешь, что дисковая подсистема способна на большее, то можешь попробовать разогнать синхронизацию.
```bash
echo '10000' > /proc/sys/dev/raid/speed_limit_min

echo '500000' > /proc/sys/dev/raid/speed_limit_max
```
Тут устанавливается минимальная и максимальная планки по скорости. Единица измерения - килобайты.

Значения по умолчанию
**min_limit = 1000**
**max_limit = 200000**

***Любой из дисков можно объявить сбойным***

```bash
sudo mdadm /dev/md0 --fail /dev/sdc1
```
Смотрим 
```bash
sudo mdadm -D /dev/md0
```
В mdstat это выголядит вот так, у проблемного диска будет преписка (F) Failed
![[Pasted image 20230311220015.png]]

Чтобы его вернуть назад, его надо сначала удалить из массива, а потом добавить обратно. Мы эти команды разбирали чуть выше.


# Режим Spare - дисков, запасных 

Представим, что ты работаешь в большой богатой корпорации и там сохранность данных и непрерывность работы серверов настолько важна, что они готовы покупать диски с запасом.

Можно добавить несколько Spare-дисков в массив. Они будут крутиться вхолостую до того момента, пока какой-то из рабочих дисков не перейдет в состояние Failed или не будет выдернут из массива вручную.

**Что для этого надо сделать?** Да просто добавить в массив лишний диск, но не выполнять расширение массива. Тогда этот лишний диск будет висеть в состоянии spare.

# Удаление массива

Для начала нужно освободить массив и размонтировать (umount). Если он кем-то используется, то удалить не получится.
Итак. Массив освобожден. Приступим.

Для начала разберем массив:
```bash
sudo mdadm -S /dev/md0
```
Массив остановлен, но его заново можно собрать на основе метаданных, сохраненных в начале каждого из дисков/разделов.

Поэтому следущим шагом будет зачистка этих данных:
```bash
sudo mdadm --zero-superblock /dev/sdb1

sudo mdadm --zero-superblock /dev/sdc1

sudo mdadm --zero-superblock /dev/sdd1

sudo wipefs --all --force /dev/sd{b1,c1,d1}
```

Теперь массив удален и собрать его больше не получится.

Source: https://yodo.im/raid

[[Linux]] 

#Диски #raid 